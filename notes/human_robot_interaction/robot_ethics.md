# Robot Ethics

[What Should We Want from a Robot Ethic?](http://www.peterasaro.org/writing/Asaro%20IRIE.pdf)
[Robot Ethics: A View from the Philosophy of Science](https://www.docenti.unina.it/downloadPub.do?tipoFile=allegatoProdotto&id=1126)

Fundamental issue of how moral responsibility should be distributed in socio-technical contexts involving robots, and how the behavior of people and robots ought to be regulated.

For the most part, the nature of robotic technology itself is not at issue, but rather the morality behind human actions and intentions exercised through the technology.

Ethics in robots includes the:
- ethical systems built into robots
- ethics of people who design and use robots
- ethics of how people treat robots

The best approach to robot ethics is one which addresses all three of these, and to do this it ought to consider robots as socio-technical systems. By doing so, it is possible to think of a continuum of agency that lies between amoral and fully autonomous moral agents. Thus, robots might move gradually along this continuum as they acquire greater capabilities and ethical sophistication.

It might also be that this might be best addressed by looking to legal theory, rather than moral theory because our overarching interest in robot ethics ought to be the practical one of preventing robots from doing harm, as well as preventing humans from unjustly avoiding responsibility for their actions.

### Types of Moral Agents

All robots are already "agents," namely causal agents, however, they are not considered to be moral agents in the sense that they are not held responsible for their actions.

Moral agents adhere to a system of ethics when they employ that system in choosing which actions they will take and which they will refrain from taking. Moral agents are also causal agents since they have the ability to take actions on the basis of making choices.

Immoral agents choose badly, go against their ethical systems, or adhere to an illegitimate or substandard system.

The system is amoral if there is no choice made or no ethical system employed.

### Types of Moral Systems

#### Dice Roll

A decision could be made by rolling a set of dice or drawing lots. It is not an issue of the morality of the decider, but rather the moral weight of the choice once made.

#### Sophisticated Decision-Making System

Provide a system with the ability to do certain kinds of ethical reasoning; to assign certain values to outcomes or to follow certain principles. Involves human building an ethical system into the robot. Called `robots with moral intelligence`.

#### Dynamic Moral Intelligence System

If a robot was built to learn new ethical lessons, develop their moral sense or even evolve their own ethical systems, we might call these `robots with dynamic moral intelligence`.

#### Full Autonomous Moral Agency

Full moral agency might require any number of further elements such as consciousness, self-awareness, the ability to feel pain or fear death, reflexive deliberation and evaluation of its own ethical system and moral judgements.

### Responsibilities and Agency in Socio-Technical Systems

The primary aim of robot ethics is to develop the means to prevent robots from doing harm to people, to the environment and to people's feelings.

In deciding moral responsibility, it is useful to view legal strategies for non-human legal entities, such as corporations. Corporations can be held legally responsible for their practices and products, through liability laws and lawsuits.

We are likely to follow the perspective of legal responsibility because:
- It is likely that legal requirements will be how robotics engineers will find themselves initially compelled to build ethical robots, and so the legal framework will structure those pressures and their technological solutions
- The legal framework provides a practical system for understanding agency and responsibility, so we will not need to wait for a final resolution of which moral theory is "right" or what moral agency "really is" in order to begin to address the ethical issues facing robotics.

Real moral complexity comes from trying to resolve moral dilemmas; choices in which different perspectives on a situation would endorse making different decisions.
