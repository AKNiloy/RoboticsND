# Robot Ethics

[What Should We Want from a Robot Ethic?](http://www.peterasaro.org/writing/Asaro%20IRIE.pdf)
[Robot Ethics: A View from the Philosophy of Science](https://www.docenti.unina.it/downloadPub.do?tipoFile=allegatoProdotto&id=1126)

Fundamental issue of how moral responsibility should be distributed in socio-technical contexts involving robots, and how the behavior of people and robots ought to be regulated.

For the most part, the nature of robotic technology itself is not at issue, but rather the morality behind human actions and intentions exercised through the technology.

Ethics in robots includes the:
- ethical systems built into robots
- ethics of people who design and use robots
- ethics of how people treat robots

The best approach to robot ethics is one which addresses all three of these, and to do this it ought to consider robots as socio-technical systems. By doing so, it is possible to think of a continuum of agency that lies between amoral and fully autonomous moral agents. Thus, robots might move gradually along this continuum as they acquire greater capabilities and ethical sophistication.

It might also be that this might be best addressed by looking to legal theory, rather than moral theory because our overarching interest in robot ethics ought to be the practical one of preventing robots from doing harm, as well as preventing humans from unjustly avoiding responsibility for their actions.

### Types of Moral Agents

All robots are already "agents," namely causal agents, however, they are not considered to be moral agents in the sense that they are not held responsible for their actions.

Moral agents adhere to a system of ethics when they employ that system in choosing which actions they will take and which they will refrain from taking. Moral agents are also causal agents since they have the ability to take actions on the basis of making choices.

Immoral agents choose badly, go against their ethical systems, or adhere to an illegitimate or substandard system.

The system is amoral if there is no choice made or no ethical system employed.

### Types of Moral Systems

#### Dice Roll

A decision could be made by rolling a set of dice or drawing lots. It is not an issue of the morality of the decider, but rather the moral weight of the choice once made.

#### Sophisticated Decision-Making System

Provide a system with the ability to do certain kinds of ethical reasoning; to assign certain values to outcomes or to follow certain principles. Involves human building an ethical system into the robot. Called `robots with moral intelligence`.

#### Dynamic Moral Intelligence System

If a robot was built to learn new ethical lessons, develop their moral sense or even evolve their own ethical systems, we might call these `robots with dynamic moral intelligence`.

#### Full Autonomous Moral Agency

Full moral agency might require any number of further elements such as consciousness, self-awareness, the ability to feel pain or fear death, reflexive deliberation and evaluation of its own ethical system and moral judgements.

### Responsibilities and Agency in Socio-Technical Systems

The primary aim of robot ethics is to develop the means to prevent robots from doing harm to people, to the environment and to people's feelings.

In deciding moral responsibility, it is useful to view legal strategies for non-human legal entities, such as corporations. Corporations can be held legally responsible for their practices and products, through liability laws and lawsuits.

We are likely to follow the perspective of legal responsibility because:
- It is likely that legal requirements will be how robotics engineers will find themselves initially compelled to build ethical robots, and so the legal framework will structure those pressures and their technological solutions
- The legal framework provides a practical system for understanding agency and responsibility, so we will not need to wait for a final resolution of which moral theory is "right" or what moral agency "really is" in order to begin to address the ethical issues facing robotics.

Real moral complexity comes from trying to resolve moral dilemmas; choices in which different perspectives on a situation would endorse making different decisions.

### Robot-Environment Interactions

It is important to identify and control environmental disturbing factors that jeopardize the normal working of robotic systems.

A heuristic strategy that is often applied to address this relies on closed-world assumptions:
- One models robotic behaviors in closed worlds that are predictable with known and well tested regularities
- One attempts to enforce (in the environments in which the robots will be actually immersed) the easily predictable conditions of those ideal closed worlds

This closed world heuristic is often used in industrial automation where either a robot "segregation" policy is enforced (isolate robots from people), or the interactions are rigidly regimented.

These interactions will continue to be a large issue as more robots are introduced in non-industrial areas. This is important to control and understand because this is relevant to autonomy, responsibility and liability issues.

*The environments in which robots are supposed to act become more dynamic and less readily predictable as one progressively moves from industrial robotics towards the current frontiers of service and personal robotics.*

### Militarized Robots

When is it alright for robots to be militarized?

Soldiers must be able to recognize surrender gestures and be able to tell  bystanders apart from foes.

If one knows that a system deployed in the battlefield is unable to discriminate between between the innocent and the enemy, then one no longer has a rational basis for distinguishing between the goals one pursues by deploying these robotic systems in the battlefield and their alleged side effects. The proposed criterion should be acceptable from consequentialist standpoints in ethics, too: while the killing of the innocent may bring short-term advantages in a war, it is likely to induce long-term resentments in the enemy, whose expected consequences should be properly taken into account in order to minimize the loss of human lives, the length of the conflict, etc.
